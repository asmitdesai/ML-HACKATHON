{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "158a1b24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to process Data/corpus.txt...\n",
      "Corpus processing complete.\n",
      "Found words for 24 different lengths.\n",
      "\n",
      "--- Word Counts Per Length (Sample) ---\n",
      "Length 1: 46 words\n",
      "Length 2: 84 words\n",
      "Length 3: 388 words\n",
      "Length 4: 1169 words\n",
      "Length 5: 2340 words\n",
      "Length 20: 40 words\n",
      "Length 21: 16 words\n",
      "Length 22: 8 words\n",
      "Length 23: 3 words\n",
      "Length 24: 1 words\n",
      "\n",
      "Example (first 5 words of length 7):\n",
      "['tunable', 'bajardo', 'emprise', 'frogbit', 'justina']\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "import os\n",
    "corpus_file = os.path.join('Data', 'corpus.txt')\n",
    "\n",
    "# Use a defaultdict to automatically handle new list creation\n",
    "words_by_length = collections.defaultdict(list)\n",
    "\n",
    "print(f\"Starting to process {corpus_file}...\")\n",
    "\n",
    "# Check if the file exists before trying to open it\n",
    "if not os.path.exists(corpus_file):\n",
    "    print(f\"Error: The file '{corpus_file}' was not found.\")\n",
    "    print(\"Please make sure your repository structure is correct.\")\n",
    "else:\n",
    "    with open(corpus_file, 'r') as f:\n",
    "        for line in f:\n",
    "            # 1. Strip leading/trailing whitespace and convert to lowercase\n",
    "            word = line.strip().lower()\n",
    "            \n",
    "            # 2. Filter out any lines that aren't purely alphabetic\n",
    "            if word.isalpha():\n",
    "                # 3. Group the word by its length\n",
    "                words_by_length[len(word)].append(word)\n",
    "\n",
    "    print(\"Corpus processing complete.\")\n",
    "\n",
    "    print(f\"Found words for {len(words_by_length)} different lengths.\")\n",
    "    \n",
    "    lengths_sorted = sorted(words_by_length.keys())\n",
    "    \n",
    "    if lengths_sorted:\n",
    "        print(\"\\n--- Word Counts Per Length (Sample) ---\")\n",
    "        sample_lengths = lengths_sorted[:5] + lengths_sorted[-5:]\n",
    "        for length in sample_lengths:\n",
    "            print(f\"Length {length}: {len(words_by_length[length])} words\")\n",
    "            \n",
    "        if 7 in words_by_length:\n",
    "            print(\"\\nExample (first 5 words of length 7):\")\n",
    "            print(words_by_length[7][:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fd6bed0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting HMM training...\n",
      "\n",
      "HMM training complete.\n",
      "Trained 23 models (one for each word length).\n",
      "Models saved to 'hmm_models.pkl' successfully.\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import pickle\n",
    "import collections\n",
    "from collections import Counter\n",
    "\n",
    "# --- Utility: Convert defaultdicts to normal dicts for pickling ---\n",
    "def to_dict(obj):\n",
    "    \"\"\"Recursively convert defaultdicts to normal dicts for pickling.\"\"\"\n",
    "    if isinstance(obj, collections.defaultdict):\n",
    "        obj = {k: to_dict(v) for k, v in obj.items()}\n",
    "    elif isinstance(obj, dict):\n",
    "        obj = {k: to_dict(v) for k, v in obj.items()}\n",
    "    return obj\n",
    "\n",
    "# --- HMM Configuration ---\n",
    "models_output_file = 'hmm_models.pkl'  # Output file for trained models\n",
    "ALPHABET = list(string.ascii_lowercase)\n",
    "NUM_CLASSES = len(ALPHABET)\n",
    "\n",
    "print(\"\\nStarting HMM training...\")\n",
    "\n",
    "# Main model storage.\n",
    "# Format: { length: {'pi': {...}, 'A': {...}} }\n",
    "hmm_models = {}\n",
    "\n",
    "for length, word_list in words_by_length.items():\n",
    "    if not word_list or length < 2:\n",
    "        continue \n",
    "\n",
    "    total_words = len(word_list)\n",
    "\n",
    "    # --- Calculate Pi (Initial Probabilities) ---\n",
    "    pi_counts = Counter(word[0] for word in word_list)\n",
    "    pi_probs = {char: (pi_counts[char] + 1) / (total_words + NUM_CLASSES)\n",
    "                for char in ALPHABET}\n",
    "\n",
    "    # --- Calculate A (Transition Probabilities) ---\n",
    "    A_counts = collections.defaultdict(lambda: collections.defaultdict(int))\n",
    "    A_totals = collections.defaultdict(int)\n",
    "\n",
    "    for word in word_list:\n",
    "        for i in range(length - 1):\n",
    "            prev_char = word[i]\n",
    "            next_char = word[i + 1]\n",
    "            A_counts[prev_char][next_char] += 1\n",
    "            A_totals[prev_char] += 1\n",
    "\n",
    "    A_probs = collections.defaultdict(lambda: collections.defaultdict(float))\n",
    "    for prev_char in ALPHABET:\n",
    "        total_prev_transitions = A_totals[prev_char]\n",
    "        for next_char in ALPHABET:\n",
    "            count = A_counts[prev_char][next_char]\n",
    "            A_probs[prev_char][next_char] = (count + 1) / (total_prev_transitions + NUM_CLASSES)\n",
    "\n",
    "    hmm_models[length] = {\n",
    "        'pi': pi_probs,\n",
    "        'A': A_probs\n",
    "    }\n",
    "\n",
    "print(\"\\nHMM training complete.\")\n",
    "print(f\"Trained {len(hmm_models)} models (one for each word length).\")\n",
    "\n",
    "# --- Convert all defaultdicts to dicts before saving ---\n",
    "hmm_models_clean = to_dict(hmm_models)\n",
    "\n",
    "# --- Save Models ---\n",
    "with open(models_output_file, 'wb') as f:\n",
    "    pickle.dump(hmm_models_clean, f)\n",
    "\n",
    "print(f\"Models saved to '{models_output_file}' successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8e9defbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully saved trained models to 'hmm_models.pkl'\n"
     ]
    }
   ],
   "source": [
    "# --- Phase 3: Save models to disk for later use ---\n",
    "import collections\n",
    "\n",
    "def to_dict(obj):\n",
    "    \"\"\"Recursively convert defaultdicts to normal dicts for pickling.\"\"\"\n",
    "    if isinstance(obj, collections.defaultdict):\n",
    "        obj = {k: to_dict(v) for k, v in obj.items()}\n",
    "    elif isinstance(obj, dict):\n",
    "        obj = {k: to_dict(v) for k, v in obj.items()}\n",
    "    return obj\n",
    "\n",
    "# Convert defaultdicts before saving\n",
    "hmm_models_clean = to_dict(hmm_models)\n",
    "\n",
    "# Use pickle to serialize the main 'hmm_models' dict\n",
    "try:\n",
    "    with open(models_output_file, 'wb') as f:\n",
    "        pickle.dump(hmm_models_clean, f)\n",
    "    print(f\"Successfully saved trained models to '{models_output_file}'\")\n",
    "except Exception as e:\n",
    "    print(f\"Error saving models: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3c2941ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Verification: Loaded 23 models ---\n",
      "\n",
      "Checking model for word length = 7...\n",
      "\n",
      "Initial Probabilities (Pi) sample:\n",
      "P('s'...) = 0.109208\n",
      "P('z'...) = 0.005840\n",
      "\n",
      "Transition Probabilities (A) sample:\n",
      "P('u' | 'q') = 0.614583\n",
      "P('x' | 'q') = 0.010417\n",
      "Sum of P(* | 'a') = 1.000000\n",
      "Sum of P(* | 'q') = 1.000000\n",
      "Sum of P(* | 's') = 1.000000\n"
     ]
    }
   ],
   "source": [
    "# --- Phase 4: Verification (Load models & check) ---\n",
    "import pickle\n",
    "\n",
    "try:\n",
    "    with open(models_output_file, 'rb') as f:\n",
    "        loaded_models = pickle.load(f)\n",
    "\n",
    "    print(f\"\\n--- Verification: Loaded {len(loaded_models)} models ---\")\n",
    "\n",
    "    # Pick a common word length (e.g., 7)\n",
    "    test_length = 7\n",
    "    if test_length in loaded_models:\n",
    "        model = loaded_models[test_length]\n",
    "        print(f\"\\nChecking model for word length = {test_length}...\")\n",
    "\n",
    "        # --- Initial Probabilities ---\n",
    "        print(\"\\nInitial Probabilities (Pi) sample:\")\n",
    "        print(f\"P('s'...) = {model['pi'].get('s', 0):.6f}\")  # likely higher\n",
    "        print(f\"P('z'...) = {model['pi'].get('z', 0):.6f}\")  # likely lower\n",
    "\n",
    "        # --- Transition Probabilities ---\n",
    "        print(\"\\nTransition Probabilities (A) sample:\")\n",
    "        q_row = model['A'].get('q', {})\n",
    "        if q_row:\n",
    "            print(f\"P('u' | 'q') = {q_row.get('u', 0):.6f}\")\n",
    "            print(f\"P('x' | 'q') = {q_row.get('x', 0):.6f}\")\n",
    "        else:\n",
    "            print(\"No transitions found for 'q' (possibly no words with 'q').\")\n",
    "\n",
    "        # --- Sanity check: normalization ---\n",
    "        for ch in ['a', 'q', 's']:\n",
    "            total_prob = sum(model['A'][ch].values())\n",
    "            print(f\"Sum of P(* | '{ch}') = {total_prob:.6f}\")\n",
    "            if abs(total_prob - 1.0) > 1e-3:\n",
    "                print(f\"Warning: probabilities for '{ch}' not normalized!\")\n",
    "\n",
    "    else:\n",
    "        print(f\"No model found for word length = {test_length}.\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Verification failed: '{models_output_file}' not found.\")\n",
    "except Exception as e:\n",
    "    print(f\"Verification failed: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0e0f4ce8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded HMM models from 'hmm_models.pkl'.\n",
      "Loaded 23 models for lengths: [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24]\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import string\n",
    "import os\n",
    "\n",
    "# --- Constants ---\n",
    "ALPHABET = list(string.ascii_lowercase)\n",
    "ALPHABET_SET = set(ALPHABET)\n",
    "MODELS_FILE = 'hmm_models.pkl'\n",
    "\n",
    "# --- Load Models ---\n",
    "hmm_models = None # Will hold all trained HMMs (pi, A)\n",
    "\n",
    "try:\n",
    "    with open(MODELS_FILE, 'rb') as f:\n",
    "        hmm_models = pickle.load(f)\n",
    "    print(f\"Successfully loaded HMM models from '{MODELS_FILE}'.\")\n",
    "    print(f\"Loaded {len(hmm_models)} models for lengths: {sorted(hmm_models.keys())}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: Model file '{MODELS_FILE}' not found.\")\n",
    "    print(\"Please run the training cell (Cell 2) first.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading models: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f4bce9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "def get_letter_probs(masked_word, guessed_letters, smoothing=1e-8):\n",
    "    \"\"\"\n",
    "    Computes posterior probabilities for each unguessed letter \n",
    "    given the current masked word state using a trained HMM model.\n",
    "    \n",
    "    Args:\n",
    "        masked_word (str): Current state of the word (e.g., \"_ppl_\").\n",
    "        guessed_letters (set): Letters already guessed (e.g., {'p', 'l', 'e'}).\n",
    "        smoothing (float): Small constant to avoid zero probabilities.\n",
    "\n",
    "    Returns:\n",
    "        dict: Normalized probabilities {letter: probability} for each unguessed letter.\n",
    "    \"\"\"\n",
    "    # --- Setup ---\n",
    "    L = len(masked_word)\n",
    "    if not hmm_models or L not in hmm_models:\n",
    "        print(f\"[WARN] No HMM model found for length {L}. Using uniform distribution.\")\n",
    "        unguessed = [c for c in ALPHABET if c not in guessed_letters]\n",
    "        return {c: 1.0 / len(unguessed) for c in unguessed} if unguessed else {}\n",
    "\n",
    "    model = hmm_models[L]\n",
    "    pi, A = model['pi'], model['A']\n",
    "\n",
    "    # --- Step 1: Build Emission Probabilities (B matrix) ---\n",
    "    B = []\n",
    "    for obs in masked_word:\n",
    "        emission = {}\n",
    "        for char in ALPHABET:\n",
    "            if obs == '_':\n",
    "                emission[char] = 1.0 if char not in guessed_letters else 0.0\n",
    "            else:\n",
    "                emission[char] = 1.0 if char == obs else 0.0\n",
    "        B.append(emission)\n",
    "\n",
    "    # --- Step 2: Forward Pass ---\n",
    "    alpha = [{} for _ in range(L)]\n",
    "    for char in ALPHABET:\n",
    "        alpha[0][char] = pi[char] * B[0][char] + smoothing\n",
    "\n",
    "    for t in range(1, L):\n",
    "        for char_next in ALPHABET:\n",
    "            alpha[t][char_next] = sum(\n",
    "                alpha[t-1][char_prev] * A[char_prev][char_next]\n",
    "                for char_prev in ALPHABET\n",
    "            ) * B[t][char_next] + smoothing\n",
    "\n",
    "    prob_obs = sum(alpha[L-1].values())\n",
    "    if prob_obs <= 0:\n",
    "        print(\"[WARN] Observation probability is zero. Falling back to uniform.\")\n",
    "        unguessed = [c for c in ALPHABET if c not in guessed_letters]\n",
    "        return {c: 1.0 / len(unguessed) for c in unguessed} if unguessed else {}\n",
    "\n",
    "    # --- Step 3: Backward Pass ---\n",
    "    beta = [{char: 1.0 for char in ALPHABET} for _ in range(L)]\n",
    "    for t in range(L - 2, -1, -1):\n",
    "        for char_prev in ALPHABET:\n",
    "            beta[t][char_prev] = sum(\n",
    "                A[char_prev][char_next] * B[t + 1][char_next] * beta[t + 1][char_next]\n",
    "                for char_next in ALPHABET\n",
    "            ) + smoothing\n",
    "\n",
    "    # --- Step 4: Compute Posterior Probabilities ---\n",
    "    posterior = collections.defaultdict(float)\n",
    "    for t, obs in enumerate(masked_word):\n",
    "        if obs == '_':\n",
    "            for char in ALPHABET:\n",
    "                if char not in guessed_letters:\n",
    "                    posterior[char] += (alpha[t][char] * beta[t][char]) / prob_obs\n",
    "\n",
    "    total = sum(posterior.values())\n",
    "    if total <= 0:\n",
    "        return {c: 0.0 for c in ALPHABET if c not in guessed_letters}\n",
    "\n",
    "    return {char: prob / total for char, prob in posterior.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "532c956e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Test the 'get_letter_probs' function ---\n",
    "\n",
    "print(\"\\n========== HMM Oracle Verification ==========\\n\")\n",
    "\n",
    "def run_test(masked, guessed, expected_top=None):\n",
    "    \"\"\"Helper function for testing get_letter_probs.\"\"\"\n",
    "    print(f\"Masked word: {masked}\")\n",
    "    print(f\"Guessed letters: {sorted(list(guessed))}\")\n",
    "    probs = get_letter_probs(masked, guessed)\n",
    "\n",
    "    if not probs:\n",
    "        print(\"No probabilities returned.\")\n",
    "        print(\"-\" * 50)\n",
    "        return\n",
    "\n",
    "    # Check that probabilities sum to ~1\n",
    "    total_prob = sum(probs.values())\n",
    "    print(f\"Sum of probabilities: {total_prob:.6f}\")\n",
    "\n",
    "    # Sort by descending probability\n",
    "    sorted_probs = sorted(probs.items(), key=lambda x: x[1], reverse=True)\n",
    "    top_3 = sorted_probs[:3]\n",
    "\n",
    "    print(f\"Top 3 predicted letters: {top_3}\")\n",
    "\n",
    "    # Optionally highlight expected letter\n",
    "    if expected_top:\n",
    "        rank = next((i+1 for i, (c, _) in enumerate(sorted_probs) if c == expected_top), None)\n",
    "        if rank:\n",
    "            print(f\"'{expected_top}' found at rank {rank} with P={probs[expected_top]:.4f}\")\n",
    "        else:\n",
    "            print(f\"Expected letter '{expected_top}' not found among probable guesses.\")\n",
    "\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "\n",
    "# --- Run Tests ---\n",
    "print(\"Running HMM tests...\\n\")\n",
    "\n",
    "run_test(masked=\"appl_\", guessed={'a', 'p', 'l'}, expected_top='e')\n",
    "run_test(masked=\"pr_ject\", guessed={'p', 'r', 'j', 'e', 'c', 't'}, expected_top='o')\n",
    "\n",
    "print(\"\\n========== Tests Complete ==========\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
