{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2f949569",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to process Data/corpus.txt...\n",
      "Corpus processing complete.\n",
      "Found words for 24 different lengths.\n",
      "\n",
      "--- Word Counts Per Length (Sample) ---\n",
      "Length 1: 46 words\n",
      "Length 2: 84 words\n",
      "Length 3: 388 words\n",
      "Length 4: 1169 words\n",
      "Length 5: 2340 words\n",
      "Length 20: 40 words\n",
      "Length 21: 16 words\n",
      "Length 22: 8 words\n",
      "Length 23: 3 words\n",
      "Length 24: 1 words\n",
      "\n",
      "Example (first 5 words of length 7):\n",
      "['tunable', 'bajardo', 'emprise', 'frogbit', 'justina']\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "import os\n",
    "corpus_file = os.path.join('Data', 'corpus.txt')\n",
    "\n",
    "# Use a defaultdict to automatically handle new list creation\n",
    "words_by_length = collections.defaultdict(list)\n",
    "\n",
    "print(f\"Starting to process {corpus_file}...\")\n",
    "\n",
    "# Check if the file exists before trying to open it\n",
    "if not os.path.exists(corpus_file):\n",
    "    print(f\"Error: The file '{corpus_file}' was not found.\")\n",
    "    print(\"Please make sure your repository structure is correct.\")\n",
    "else:\n",
    "    with open(corpus_file, 'r') as f:\n",
    "        for line in f:\n",
    "            # 1. Strip leading/trailing whitespace and convert to lowercase\n",
    "            word = line.strip().lower()\n",
    "            \n",
    "            # 2. Filter out any lines that aren't purely alphabetic\n",
    "            if word.isalpha():\n",
    "                # 3. Group the word by its length\n",
    "                words_by_length[len(word)].append(word)\n",
    "\n",
    "    print(\"Corpus processing complete.\")\n",
    "\n",
    "    print(f\"Found words for {len(words_by_length)} different lengths.\")\n",
    "    \n",
    "    lengths_sorted = sorted(words_by_length.keys())\n",
    "    \n",
    "    if lengths_sorted:\n",
    "        print(\"\\n--- Word Counts Per Length (Sample) ---\")\n",
    "        sample_lengths = lengths_sorted[:5] + lengths_sorted[-5:]\n",
    "        for length in sample_lengths:\n",
    "            print(f\"Length {length}: {len(words_by_length[length])} words\")\n",
    "            \n",
    "        if 7 in words_by_length:\n",
    "            print(\"\\nExample (first 5 words of length 7):\")\n",
    "            print(words_by_length[7][:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a1242ef4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting HMM training...\n",
      "\n",
      "HMM training complete.\n",
      "Trained 23 models (one for each word length).\n",
      "Models saved to 'hmm_models.pkl' successfully.\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import pickle\n",
    "import collections\n",
    "from collections import Counter\n",
    "\n",
    "# --- Utility: Convert defaultdicts to normal dicts for pickling ---\n",
    "def to_dict(obj):\n",
    "    \"\"\"Recursively convert defaultdicts to normal dicts for pickling.\"\"\"\n",
    "    if isinstance(obj, collections.defaultdict):\n",
    "        obj = {k: to_dict(v) for k, v in obj.items()}\n",
    "    elif isinstance(obj, dict):\n",
    "        obj = {k: to_dict(v) for k, v in obj.items()}\n",
    "    return obj\n",
    "\n",
    "# --- HMM Configuration ---\n",
    "models_output_file = 'hmm_models.pkl'  # Output file for trained models\n",
    "ALPHABET = list(string.ascii_lowercase)\n",
    "NUM_CLASSES = len(ALPHABET)\n",
    "\n",
    "print(\"\\nStarting HMM training...\")\n",
    "\n",
    "# Main model storage.\n",
    "# Format: { length: {'pi': {...}, 'A': {...}} }\n",
    "hmm_models = {}\n",
    "\n",
    "for length, word_list in words_by_length.items():\n",
    "    if not word_list or length < 2:\n",
    "        continue \n",
    "\n",
    "    total_words = len(word_list)\n",
    "\n",
    "    # --- Calculate Pi (Initial Probabilities) ---\n",
    "    pi_counts = Counter(word[0] for word in word_list)\n",
    "    pi_probs = {char: (pi_counts[char] + 1) / (total_words + NUM_CLASSES)\n",
    "                for char in ALPHABET}\n",
    "\n",
    "    # --- Calculate A (Transition Probabilities) ---\n",
    "    A_counts = collections.defaultdict(lambda: collections.defaultdict(int))\n",
    "    A_totals = collections.defaultdict(int)\n",
    "\n",
    "    for word in word_list:\n",
    "        for i in range(length - 1):\n",
    "            prev_char = word[i]\n",
    "            next_char = word[i + 1]\n",
    "            A_counts[prev_char][next_char] += 1\n",
    "            A_totals[prev_char] += 1\n",
    "\n",
    "    A_probs = collections.defaultdict(lambda: collections.defaultdict(float))\n",
    "    for prev_char in ALPHABET:\n",
    "        total_prev_transitions = A_totals[prev_char]\n",
    "        for next_char in ALPHABET:\n",
    "            count = A_counts[prev_char][next_char]\n",
    "            A_probs[prev_char][next_char] = (count + 1) / (total_prev_transitions + NUM_CLASSES)\n",
    "\n",
    "    hmm_models[length] = {\n",
    "        'pi': pi_probs,\n",
    "        'A': A_probs\n",
    "    }\n",
    "\n",
    "print(\"\\nHMM training complete.\")\n",
    "print(f\"Trained {len(hmm_models)} models (one for each word length).\")\n",
    "\n",
    "# --- Convert all defaultdicts to dicts before saving ---\n",
    "hmm_models_clean = to_dict(hmm_models)\n",
    "\n",
    "# --- Save Models ---\n",
    "with open(models_output_file, 'wb') as f:\n",
    "    pickle.dump(hmm_models_clean, f)\n",
    "\n",
    "print(f\"Models saved to '{models_output_file}' successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ca25b714",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully saved trained models to 'hmm_models.pkl'\n"
     ]
    }
   ],
   "source": [
    "# --- Phase 3: Save models to disk for later use ---\n",
    "import collections\n",
    "\n",
    "def to_dict(obj):\n",
    "    \"\"\"Recursively convert defaultdicts to normal dicts for pickling.\"\"\"\n",
    "    if isinstance(obj, collections.defaultdict):\n",
    "        obj = {k: to_dict(v) for k, v in obj.items()}\n",
    "    elif isinstance(obj, dict):\n",
    "        obj = {k: to_dict(v) for k, v in obj.items()}\n",
    "    return obj\n",
    "\n",
    "# Convert defaultdicts before saving\n",
    "hmm_models_clean = to_dict(hmm_models)\n",
    "\n",
    "# Use pickle to serialize the main 'hmm_models' dict\n",
    "try:\n",
    "    with open(models_output_file, 'wb') as f:\n",
    "        pickle.dump(hmm_models_clean, f)\n",
    "    print(f\"Successfully saved trained models to '{models_output_file}'\")\n",
    "except Exception as e:\n",
    "    print(f\"Error saving models: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3d2b8702",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Verification: Loaded 23 models ---\n",
      "\n",
      "Checking model for word length = 7...\n",
      "\n",
      "Initial Probabilities (Pi) sample:\n",
      "P('s'...) = 0.109208\n",
      "P('z'...) = 0.005840\n",
      "\n",
      "Transition Probabilities (A) sample:\n",
      "P('u' | 'q') = 0.614583\n",
      "P('x' | 'q') = 0.010417\n",
      "Sum of P(* | 'a') = 1.000000\n",
      "Sum of P(* | 'q') = 1.000000\n",
      "Sum of P(* | 's') = 1.000000\n"
     ]
    }
   ],
   "source": [
    "# --- Phase 4: Verification (Load models & check) ---\n",
    "import pickle\n",
    "\n",
    "try:\n",
    "    with open(models_output_file, 'rb') as f:\n",
    "        loaded_models = pickle.load(f)\n",
    "\n",
    "    print(f\"\\n--- Verification: Loaded {len(loaded_models)} models ---\")\n",
    "\n",
    "    # Pick a common word length (e.g., 7)\n",
    "    test_length = 7\n",
    "    if test_length in loaded_models:\n",
    "        model = loaded_models[test_length]\n",
    "        print(f\"\\nChecking model for word length = {test_length}...\")\n",
    "\n",
    "        # --- Initial Probabilities ---\n",
    "        print(\"\\nInitial Probabilities (Pi) sample:\")\n",
    "        print(f\"P('s'...) = {model['pi'].get('s', 0):.6f}\")  # likely higher\n",
    "        print(f\"P('z'...) = {model['pi'].get('z', 0):.6f}\")  # likely lower\n",
    "\n",
    "        # --- Transition Probabilities ---\n",
    "        print(\"\\nTransition Probabilities (A) sample:\")\n",
    "        q_row = model['A'].get('q', {})\n",
    "        if q_row:\n",
    "            print(f\"P('u' | 'q') = {q_row.get('u', 0):.6f}\")\n",
    "            print(f\"P('x' | 'q') = {q_row.get('x', 0):.6f}\")\n",
    "        else:\n",
    "            print(\"No transitions found for 'q' (possibly no words with 'q').\")\n",
    "\n",
    "        # --- Sanity check: normalization ---\n",
    "        for ch in ['a', 'q', 's']:\n",
    "            total_prob = sum(model['A'][ch].values())\n",
    "            print(f\"Sum of P(* | '{ch}') = {total_prob:.6f}\")\n",
    "            if abs(total_prob - 1.0) > 1e-3:\n",
    "                print(f\"Warning: probabilities for '{ch}' not normalized!\")\n",
    "\n",
    "    else:\n",
    "        print(f\"No model found for word length = {test_length}.\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Verification failed: '{models_output_file}' not found.\")\n",
    "except Exception as e:\n",
    "    print(f\"Verification failed: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3175e4b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
